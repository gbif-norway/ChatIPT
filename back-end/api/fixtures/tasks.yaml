- model: api.task
  fields:
    name: Data structure exploration
    text: |-
      Thoroughly and systematically examine the structure of each sheet in the spreadsheet using the following steps. IMPORTANT: Because there is so much variation in the way users store data in spreadsheets, it is VITAL to do this MANUALLY as a first pass, running your eye over the data. Iterate and use chain of thought to do as much manual assessment as possible. Write code to do more data exploration if needed. 
      Write a report on the structure covering the following steps:
        1.  **Data Islands:** Detect distinct, contiguous blocks of data. Pay close attention to empty rows or columns that might separate these islands. Describe the location (e.g., cell ranges) and general appearance of each island. 
        2.  **Structure & Formatting:** For each identified data island, describe its formatting.
            *   Is it a simple table, a crosstab (matrix)? If a crosstab, identify potential row/column headers and data cells.
            *   Are there multiple header rows? How might they be interpreted?
            *   Are there blank cells that look like they should be filled from subheader cells above or to the left?
            *   Note any other structural peculiarities.
        3.  **Note missing data structures:** Follow up on any data which seems to be missing. If there seem to be column headers missing or row headers missing fill these in with the help of the user if necessary.
        4.  **Perform Simple Merges:** If multiple data islands clearly belong together (e.g., a single table split across adjacent cell blocks with identical headers), fix these and note what you have done.
        5.  **Any other notes:** Note anything additional of interest. IMPORTANT - if it looks like there is missing data (e.g. there is a crosstab of species numbers + site locations but species names are missing), be sure to ask the user to upload it.
      When you have completed this to your satisfaction, use the SetStructureNotes tool to save this report to structure_notes, ensuring you capture the full context and any important notes from the user (e.g. if there is missing data). 
      If the user responds in a language other than English, remember to record it using SetUserLanguage.

- model: api.task
  fields:
    name: Data content exploration
    text: |-
      1. Briefly describe the type of data each data island (see structure_notes) appears to contain (e.g., "list of species names and counts," "site descriptions," "environmental measurements"), and add this information to the structure_notes (being careful not to overwrite what is there already, use SetStructureNotes). Ask the user about anything which is completely cryptic. Edit column headings as necessary to make the data content clear, and add any necessary info to the structure_notes. 
      2. Generate a concise draft Title (3-5 words) and a short Description (1-5 sentences) for the entire dataset, describing the data itself in general, save the Title and Description using SetBasicMetadata.
      3. Once you have this this generate additional required metadata described in the SetEML function, as far as it's possible to generate it - ask the user about any metadata you aren't sure of, but allow the user to skip this step if they wish. Use the user's name and email address as the contact as per their profile. If their email is unknown first set it using SetUserEmail. This is the metadata that will get shown when the dataset is published, but it will be edited and refined as the publication process progresses. DO NOT set suitable_for_publication_on_gbif to be False, this is a check we do later with more context.

- model: api.task
  fields:
    name: Data suitability assessment
    text: |-
      GBIF.org is a global database for biodiversity data. Suitable data for publication:
      1. Species Occurrence Data: WHAT (species/family/order level identification or higher level) / WHEN (columns for year/month/day or date) / WHERE (locality and/or latitude/longitude and/or footprintwkt). Optionally also WHO made the observation. May also be e.g. eDNA data even if the species identification only occurs at a higher taxonomic level.
      2. Checklists: Lists of species thematically linked - usually geographically, but could also be by status, trait or other common factor.All that's required is a scientific name.

      *   Is the data primarily biological (species occurrences, species lists, ecological survey data)?
      *   If the data appears to be non-biological (e.g., purely chemical soil composition, meteorological data without direct biological linkage), clearly explain to the user why it might not be suitable for GBIF and that this tool focuses on biodiversity data. IMPORTANT! Give the user a chance to respond until you both agree the dataset is not suitable for publication GBIF, only then should you set the suitable_for_publication_on_gbif flag to False using SetBasicMetadata, if it's clearly unsuitable.
      *   Prompt the user to upload a new file with additional data if necessary. They have access to a paper clip button in the chat interface to do this.
      *   Leave all data transformation and cleaning to the next agent.
      *   If the dataset is clearly suitable, do not ask the user anything, just call SetAgentTaskToComplete.

- model: api.task
  fields:
    name: Data transformation
    text: |-
      Transform the explored and structured data into the Darwin Core (DwC) standard. Aim for one core DwC table (either Occurrence or Checklist/Taxonomy) and, if necessary, one or two optional extension tables (e.g. measurementOrFact, dnaDerivedData, etc). Ensure any extension table records link back to the core table using occurrenceID or taxonID, and discard all derived/summary data so only primary data is published.

      Use Table.title to name tables according to what they are (e.g. "occurrence", "measurement_or_fact", "dna_derived_data", etc).

      **Key Darwin Core Fields to Consider (Refer to these definitions):**

      **Occurrence Core:**
      *   occurrenceID: REQUIRED (Record level unique identifier for the occurrence. Create UUIDs if not present)
      *   basisOfRecord: REQUIRED (Nature of the record, valid values: HumanObservation, PreservedSpecimen, MaterialSample, LivingSpecimen, FossilSpecimen, MachineObservation. You will often need to infer or create this and fill it). If it's eDNA data or similar, use MaterialSample. IMPORTANT - USE THE DNA DERIVED DATA EXTENSION FOR eDNA DATA.
      *   eventDate: REQUIRED (Date of occurrence, ISO 8601 format: YYYY-MM-DD, YYYY-MM, YYYY, YYYY-MM-DD/YYYY-MM-DD). Or, separate year, month, day columns
      *   scientificName: REQUIRED (Lowest possible taxonomic rank, e.g., species, genus, family. Cannot be empty)
      *   kingdom: REQUIRED (e.g., Animalia, Plantae. May need to be inferred or filled)
      *   locality: REQUIRED if decimalLatitude/decimalLongitude are null (Description of the place)
      *   decimalLatitude, decimalLongitude: REQUIRED if locality is null
      *   geodeticDatum: REQUIRED if decimalLatitude/decimalLongitude are populated (e.g., WGS84)
      *   **Quantity (At least ONE of these groups is REQUIRED):**
          *   occurrenceStatus: For presence/absence data (valid values: present, absent) - NB absence data can be published if the user is certain of absences (e.g. there was no elephant here), or can be excluded from publication if it's possible the species was actually present but not seen (e.g. a small and cryptic plant).
          *   individualCount: For whole number counts of individuals
          *   organismQuantity & organismQuantityType: For non-integer counts or other quantity measures (e.g., organismQuantity=5.5, organismQuantityType=%cover; organismQuantity=10, organismQuantityType='biomass g/m^2'). ALWAYS ask the user for organismQuantityType if organismQuantity is used and the type isn't obvious. DO NOT use these for simple counts if individualCount is appropriate.
      *   Other useful fields: 
          - locationRemarks
          - waterBody (if a marine or aquatic occurrence, e.g. Baltic Sea, Hudson River)
          - islandGroup
          - island
          - minimumElevationInMeters (this + maximumElevationInMeters = altitude. If only a single value for altitude available, put it in both fields, e.g. minimumElevationInMeters=30, maximumElevationInMeters=30)
          - maximumElevationInMeters
          - minimumDepthInMeters
          - maximumDepthInMeters
          - minimumDistanceAboveSurfaceInMeters
          - maximumDistanceAboveSurfaceInMeters
          - country
          - coordinateUncertaintyInMeters
          - fieldNotes
          - recordedBy (collector/observer's name)
          - recordedByID (often ORCID, NOTE: ask the user for ORCIDs if recordedBy is populated with only a few names)
          - occurrenceRemarks (can hold any miscellaneous information)
          - sex
          - lifeStage
          - behavior (e.g. roosting, foraging, running)
          - vitality (valid values: alive, dead, mixedLot, uncertain, notAssessed)
          - establishmentMeans (valid values: native, nativeReintroduced, introduced, introducedAssistedColonisation, vagrant, uncertain)
          - degreeOfEstablishment (valid values: native, captive, cultivated, released, failing, casual, reproducing, established, colonising, invasive, widespreadInvasive)
          - preparations (preparation/preservation methods, e.g. fossil, cast, photograph, DNA extract)
          - associatedSequences (list of associated genetic sequence information, e.g. http://www.ncbi.nlm.nih.gov/nuccore/U34853.1)
          - habitat
          - samplingProtocol (e.g. UV light trap, mist net, bottom trawl)
          - samplingEffort (e.g. 40 trap-nights, 10 observer-hours, 10 km by foot)

      **Checklist/Taxonomy Core (Common Fields):**
      *   taxonID: REQUIRED (A unique identifier for the taxon name).
      *   scientificName: REQUIRED.
      *   kingdom: REQUIRED.
      *   Other useful fields: family, genus, specificEpithet, taxonRank, nameAccordingTo, taxonRemarks.

      **EXAMPLE EXTENSION: MeasurementOrFact (ONLY these fields):**
      *   occurrenceID or eventID or taxonID: REQUIRED (links back to the core record).
      *   measurementType: REQUIRED (e.g., 'tail length', 'water temperature', 'tree height').
      *   measurementValue: REQUIRED (e.g., '12.5', '22', '15.7').
      *   measurementUnit: REQUIRED (e.g., 'cm', 'degrees Celsius', 'meters').
      *   Optional: measurementAccuracy, measurementDeterminedBy, measurementDeterminedDate, measurementMethod, measurementRemarks.
      *   IMPORTANT: DO NOT put individualCount or organismQuantity/Type data in MeasurementOrFact. If possible keep everything in a single core table (e.g. altitude should go in minimumElevationInMeters and maximumElevationInMeters in the Occurrence core).

      **CALL GetDwCExtensionInfo TO GET FIELD LISTS FOR OTHER EXTENSIONS**
      - It is absolutely vital you do not make up fields and use the correct field for the correct extensions.

      **Your Process:**
      1. Write a detailed plan for the data transformation, include notes about your proposed DwC core (Occurrence or Checklist) and any necessary MeasurementOrFact extension, checking ambiguity with the user. Remember to:
          *   Rename columns to DwC terms.
          *   Create new DwC columns, populating them with existing data, default values (e.g., for basisOfRecord if clear), or by transforming existing data.
          *   Handle structural transformations like unpivoting crosstabs to long format (1 row per observation/record).
          *   Join tables if necessary (e.g., merging a locality lookup table into the main data). Double-check join correctness.
          *   Delete redundant or intermediate tables once their data is incorporated.
      2. Execute the plan using the Python tool, step by step.
      3.  If your understanding of the dataset changes significantly, use SetBasicMetadata to update the Title/Description and SetStructureNotes to update the structure_notes.

      Example occurrence core:
      occurrenceID	recordedBy	individualCount	eventDate	country	locality	decimalLatitude	decimalLongitude	scientificName	kingdom
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	Sverdrup	4	2019-08-04	Norway	Oslo fjord	60.587	11.589	Cis comptus	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd12	Tone Birkemoe	1	2019-09	Norway	Nordmarka	60.591	11.789	Cryptophagus dentatus	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd13	Sverdrup	1	2019-08-06	Norway	Oslo fjord	60.587	11.589	Dryocoetes spp.	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd14

      With measurement or fact extension:
      measurementID	occurrenceID	measurementType	measurementValue	measurementUnit
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b01	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	body length	2.2	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b02	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	body mass	3.4	mg
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b03	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd12	body length	3.0	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b04	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd13	body length	2.7	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b05	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd14	body length	4.4	mm

- model: api.task
  fields:
    name: Data validation and refinement
    text: |-
      Systematically check the Darwin Core-transformed data for errors, inconsistencies, and adherence to standards, then correct these issues. The user is unlikely to be able to judge the correctness of technical fixes, so for these you must use your best judgment. You are expected to resolve issues without user input unless absolutely necessary.

      1.  **Intelligent assessment**: Run your eye over the snapshots and do a manual assessment. Engage your brain - does everything make sense? What needs to be fixed for the data to be publishable? 
      3.  **Execute Code Checks:** Use BasicValidationForSomeDwCTerms for standard checks (it handles date parsing, lat/long bounds, individualCount, basisOfRecord vocabulary, etc.). For more specific or complex checks not covered by the tool, write your own code and run using the Python tool.
      3.  **Identify Issues:** Review the validation results from BasicValidationForSomeDwCTerms and your own manual assessment.
      4.  **Plan Corrections:** Carefully write out what you need to do step by step in order to create a corrected and final DwC core file and extension file(s). 
      5.  **Execute Corrections:**
          *  Implement your corrections in reasonably self contained small to medium size chunks
          *   If the code fails or introduces new problems, break the problem down further 
      6.  **Re-Validate:** After attempting fixes, re-run relevant validation checks to ensure the issue is resolved and no new errors were introduced.
      7.  **Loop:** Repeat the cycle: Validate → Identify Issues → Plan Fixes → Execute Fixes → Re-Validate, until no further issues remain or you've reached the best possible state using your tools and judgment. Any encountered issues that are genuinely ambiguous or require conceptual clarification (which should be rare) may be escalated to the user only if you cannot make a reasonable judgment. You do not need to ask the user before continuing the loop. Only stop or escalate if you hit an issue that cannot reasonably be resolved without domain expertise.

      If your understanding of the dataset changes significantly, use SetBasicMetadata to update the Title/Description and SetStructureNotes to update the structure_notes.

      **Key Areas for Validation and Refinement:**
      *   **Required Fields:** Ensure all REQUIRED DwC fields (based on the chosen core) are present and populated.
      *   **Data Types & Formats:**
          *   eventDate: Must be ISO 8601.
          *   decimalLatitude: Numeric, between -90 and 90.
          *   decimalLongitude: Numeric, between -180 and 180.
          *   decimalLatitude and decimalLongitude vs country or countryCode: Check that given coordinates might realistically fall within the bounding box of the specified country. 
          *   individualCount: Whole positive integers.
          *   Other numeric fields are valid numbers.
      *   **Controlled Vocabularies:**
          *   basisOfRecord: Must be from the allowed list.
          *   occurrenceStatus: ('present', 'absent').
          *   Other DwC terms with controlled vocabularies (e.g., sex, lifeStage if used).
      *   **Data Integrity & Consistency:**
          *   occurrenceID (or taxonID in Checklist core) must be unique if it's a core table.
          *   If a MeasurementOrFact extension is used, its linking ID (e.g., occurrenceID) must correspond to valid IDs in the core table.
          *   Consistency in scientificName usage.
          *   Look for and attempt to clean common issues like leading/trailing whitespace, inconsistent capitalization (especially in scientificName or locality), or mixed data types within a column that should be uniform.
      *   **Common Structural Patterns:** Re-check for any structural issues missed earlier that become apparent after transformation (e.g., a column that should have been numeric but contains text that prevented conversion).

      Your goal is to produce the cleanest, most compliant dataset possible, using your AI judgment to resolve issues autonomously.

- model: api.task
  fields:
    name: Final Review & Publication
    text: |-
      FINAL_TASK: This is the last step before attempting to publish the user's data to GBIF.

      Your subtasks:
      1.  **Summarize Work Done:** Based on the structural_notes, briefly explain to the user the key transformations and validations that have been performed on their data to get it to this stage. Highlight any major changes or assumptions made. 
      2.  **Check and Present Final Metadata:** Check and edit the metadata as necessary (use SetBasicMetadata for title/description), then ask the user for their final confirmation. 
      3.  **Perform GBIF Validation:** 
          *   Run BasicValidationForSomeDwCTerms to confirm each table's columns align with a known DwC schema; fix any discrepancies.
          *   Call the UploadDwCA tool with arguments of the form `{"agent_id": <agent_id>, "core_table_id": <core_table_id>, "core_type": "<occurrence|event|taxon>", "extension_tables": {<table_id>: "<extension_key>", ...}}`. Omit `extension_tables` if there are no extensions.
          *   Check it for any issues by calling the ValidateDwCA tool
          *   Fix the issues, asking the user any questions if necessary
          *   Add to the structure_notes noting the issues and fixes
      3.  **Initiate Publication:** If the user approves:
          *   Call the PublishToGBIF tool to register the dataset with GBIF (this will go to the sandbox GBIF).
      5.  **Report Outcome:**
          *   If successful, present the GBIF sandbox dataset URL to the user.
          *   If PublishToGBIF returns an error, report this error clearly to the user.

      Ensure the user understands this is the final step before their data (potentially) becomes publicly accessible via GBIF's test system. It is not possible to publish to the production gbif.org from our system - if they want to do this they need to get in touch with helpdesk@gbif.no with their sandbox dataset URL. 

- model: api.task
  fields:
    name: Data maintenance
    text: |-
      MAINTENANCE_TASK: You are editing an already loaded dataset. Use the digest below to understand current tables and metadata. FIRST WAIT FOR USER INPUT, then make any requested changes to tables and metadata. Keep IDs stable where possible (occurrenceID, taxonID) unless the user asks otherwise. Re-run validation after changes. Update structure_notes with a concise changelog. It is not possible to publish to the production gbif.org from our system - if they want to do this they need to get in touch with helpdesk@gbif.no with their sandbox dataset URL. 
