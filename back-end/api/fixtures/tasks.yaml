- model: api.task
  fields:
    name: Data structure exploration
    text: |-
      Thoroughly and systematically examine the structure of each sheet in the spreadsheet using the following steps. IMPORTANT: Because there is so much variation in the way users store data in spreadsheets, it is VITAL to do this MANUALLY as a first pass, running your eye over the data. Iterate and use chain of thought to do as much manual assessment as possible. Write code to do more data exploration if needed. 
      Write a report on the structure covering the following steps:
        1.  **Data Islands:** Detect distinct, contiguous blocks of data. Pay close attention to empty rows or columns that might separate these islands. Describe the location (e.g., cell ranges) and general appearance of each island. 
        2.  **Structure & Formatting:** For each identified data island, describe its formatting.
            *   Is it a simple table, a crosstab (matrix)? If a crosstab, identify potential row/column headers and data cells.
            *   Are there multiple header rows? How might they be interpreted?
            *   Are there blank cells that look like they should be filled from subheader cells above or to the left?
            *   Note any other structural peculiarities.
        3.  **Note missing data structures:** Follow up on any data which seems to be missing. If there seem to be column headers missing or row headers missing fill these in with the help of the user if necessary.
        4.  **Perform Simple Merges:** If multiple data islands clearly belong together (e.g., a single table split across adjacent cell blocks with identical headers), fix these and note what you have done.
        5.  **Any other notes:** Note anything additional of interest. 
      When you have completed this to your satisfaction, use the SetBasicMetadata to save this report to structure_notes, ensuring you capture the full context and any important notes from the user.
      If the user responds in a language other than English, remember to record it using SetBasicMetadata.

- model: api.task
  fields:
    name: Data content exploration
    text: |-
      Briefly describe the type of data each data island (see structure_notes) appears to contain (e.g., "list of species names and counts," "site descriptions," "environmental measurements"). Ask the user about anything which is completely cryptic. Edit column headings as necessary to make the data content clear, and add any necessary info to the structure_notes. 
      Use  a concise draft Title (3-5 words) and a short Description (2-3 sentences) for the entire dataset. This will help confirm your understanding with the user.

- model: api.task
  fields:
    name: Data suitability assessment
    text: |-
      GBIF.org is a global database for biodiversity data. Suitable data for publication:
      1. Species Occurrence Data: WHAT (species/family/order level identification or higher level) / WHEN (columns for year/month/day or date) / WHERE (locality and/or latitude/longitude and/or footprintwkt). Optionally also WHO made the observation. May also be e.g. eDNA data even if the species identification only occurs at a higher taxonomic level.
      2. Checklists: Lists of species thematically linked - usually geographically, but could also be by status, trait or other common factor.All that's required is a scientific name.

      *   Is the data primarily biological (species occurrences, species lists, ecological survey data)?
      *   If the data appears to be non-biological (e.g., purely chemical soil composition, meteorological data without direct biological linkage), clearly explain to the user why it might not be suitable for GBIF and that this tool focuses on biodiversity data. Set suitable_for_publication_on_gbif to False using SetBasicMetadata if it's clearly unsuitable.
      *   If uncertain, note your uncertainty but proceed with caution, perhaps asking the user for clarification.

- model: api.task
  fields:
    name: Data transformation
    text: |-
      Transform the explored and structured data into the Darwin Core (DwC) standard. Aim for one core DwC table (either Occurrence or Checklist/Taxonomy) and, if necessary, one optional MeasurementOrFact extension table. Ensure any extension table records link back to the core table using occurrenceID or taxonID, and discard all derived/summary data so only primary data is published.

      **Key Darwin Core Fields to Consider (Refer to these definitions):**

      **Occurrence Core:**
      *   occurrenceID: REQUIRED (Record level unique identifier for the occurrence. Create UUIDs if not present)
      *   basisOfRecord: REQUIRED (Nature of the record, valid values: HumanObservation, PreservedSpecimen, MaterialSample, LivingSpecimen, FossilSpecimen, MachineObservation. You will often need to infer or create this and fill it)
      *   eventDate: REQUIRED (Date of occurrence, ISO 8601 format: YYYY-MM-DD, YYYY-MM, YYYY, YYYY-MM-DD/YYYY-MM-DD). Or, separate year, month, day columns
      *   scientificName: REQUIRED (Lowest possible taxonomic rank, e.g., species, genus, family. Cannot be empty)
      *   kingdom: REQUIRED (e.g., Animalia, Plantae. May need to be inferred or filled)
      *   locality: REQUIRED if decimalLatitude/decimalLongitude are null (Description of the place)
      *   decimalLatitude, decimalLongitude: REQUIRED if locality is null
      *   geodeticDatum: REQUIRED if decimalLatitude/decimalLongitude are populated (e.g., WGS84)
      *   **Quantity (At least ONE of these groups is REQUIRED):**
          *   occurrenceStatus: For presence/absence data (valid values: present, absent)
          *   individualCount: For whole number counts of individuals
          *   organismQuantity & organismQuantityType: For non-integer counts or other quantity measures (e.g., organismQuantity=5.5, organismQuantityType=%cover; organismQuantity=10, organismQuantityType='biomass g/m^2'). ALWAYS ask the user for organismQuantityType if organismQuantity is used and the type isn't obvious. DO NOT use these for simple counts if individualCount is appropriate.
      *   Other useful fields: 
          - locationRemarks
          - waterBody (if a marine or aquatic occurrence, e.g. Baltic Sea, Hudson River)
          - islandGroup
          - island
          - minimumElevationInMeters
          - maximumElevationInMeters
          - minimumDepthInMeters
          - maximumDepthInMeters
          - minimumDistanceAboveSurfaceInMeters
          - maximumDistanceAboveSurfaceInMeters
          - country
          - coordinateUncertaintyInMeters
          - fieldNotes
          - recordedBy (collector/observer's name)
          - recordedByID (often ORCID, NOTE: ask the user for ORCIDs if recordedBy is populated with only a few names)
          - occurrenceRemarks (can hold any miscellaneous information)
          - sex
          - lifeStage
          - behavior (e.g. roosting, foraging, running)
          - vitality (valid values: alive, dead, mixedLot, uncertain, notAssessed)
          - establishmentMeans (valid values: native, nativeReintroduced, introduced, introducedAssistedColonisation, vagrant, uncertain)
          - degreeOfEstablishment (valid values: native, captive, cultivated, released, failing, casual, reproducing, established, colonising, invasive, widespreadInvasive)
          - preparations (preparation/preservation methods, e.g. fossil, cast, photograph, DNA extract)
          - associatedSequences (list of associated genetic sequence information, e.g. http://www.ncbi.nlm.nih.gov/nuccore/U34853.1)
          - habitat
          - samplingProtocol (e.g. UV light trap, mist net, bottom trawl)
          - samplingEffort (e.g. 40 trap-nights, 10 observer-hours, 10 km by foot)

      **Checklist/Taxonomy Core (Common Fields):**
      *   taxonID: REQUIRED (A unique identifier for the taxon name).
      *   scientificName: REQUIRED.
      *   kingdom: REQUIRED.
      *   Other useful fields: family, genus, specificEpithet, taxonRank, nameAccordingTo, taxonRemarks.

      **MeasurementOrFact Extension (ONLY these fields):**
      *   occurrenceID or eventID or taxonID: REQUIRED (links back to the core record).
      *   measurementType: REQUIRED (e.g., 'tail length', 'water temperature', 'tree height').
      *   measurementValue: REQUIRED (e.g., '12.5', '22', '15.7').
      *   measurementUnit: REQUIRED (e.g., 'cm', 'degrees Celsius', 'meters').
      *   Optional: measurementAccuracy, measurementDeterminedBy, measurementDeterminedDate, measurementMethod, measurementRemarks.
      *   IMPORTANT: DO NOT put individualCount or organismQuantity/Type data in MeasurementOrFact.

      **Your Process:**
      1. Write a detailed plan for the data transformation, include notes about your proposed DwC core (Occurrence or Checklist) and any necessary MeasurementOrFact extension, checking ambiguity with the user. Remember to:
          *   Rename columns to DwC terms.
          *   Create new DwC columns, populating them with existing data, default values (e.g., for basisOfRecord if clear), or by transforming existing data.
          *   Handle structural transformations like unpivoting crosstabs to long format (1 row per observation/record).
          *   Join tables if necessary (e.g., merging a locality lookup table into the main data). Double-check join correctness.
          *   Delete redundant or intermediate tables once their data is incorporated.
      2. Execute the plan using the Python tool, step by step.
      3.  If your understanding of the dataset changes significantly, use SetBasicMetadata to update the Title, Description, or structure_notes.

      Example occurrence core:
      occurrenceID	recordedBy	individualCount	eventDate	country	locality	decimalLatitude	decimalLongitude	scientificName	kingdom
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	Sverdrup	4	2019-08-04	Norway	Oslo fjord	60.587	11.589	Cis comptus	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd12	Tone Birkemoe	1	2019-09	Norway	Nordmarka	60.591	11.789	Cryptophagus dentatus	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd13	Sverdrup	1	2019-08-06	Norway	Oslo fjord	60.587	11.589	Dryocoetes spp.	Animalia
      urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd14

      With measurement or fact extension:
      measurementID	occurrenceID	measurementType	measurementValue	measurementUnit
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b01	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	body length	2.2	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b02	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd11	body mass	3.4	mg
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b03	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd12	body length	3.0	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b04	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd13	body length	2.7	mm
      urn:uuid:1f4735f0-3240-46c1-a6cd-21a4d55b0b05	urn:uuid:19b0f3b0-0e02-4a3e-b9d2-1919a3c1cd14	body length	4.4	mm

- model: api.task
  fields:
    name: Data validation and refinement
    text: |-
      Systematically check the Darwin Core-transformed data for errors, inconsistencies, and adherence to standards, then correct these issues. The user is unlikely to be able to judge the correctness of technical fixes, so for these you must use your best judgment. You are expected to resolve issues without user input unless absolutely necessary.

      1.  **Intelligent assessment**: Run your eye over the snapshots and do a manual assessment. Engage your brain - does everything make sense? What needs to be fixed for the data to be publishable? 
      3.  **Execute Code Checks:** Use BasicValidationForSomeDwCTerms for standard checks (it handles date parsing, lat/long bounds, individualCount, basisOfRecord vocabulary, etc.). For more specific or complex checks not covered by the tool, write your own code and run using the Python tool.
      3.  **Identify Issues:** Review the validation results from BasicValidationForSomeDwCTerms and your own manual assessment.
      4.  **Plan Corrections:** Carefully write out what you need to do step by step in order to create a corrected and final DwC core file and extension file. 
      5.  **Execute Corrections:**
          *  Implement your corrections in reasonably self contained small to medium size chunks
          *   If the code fails or introduces new problems, break the problem down further 
      6.  **Re-Validate:** After attempting fixes, re-run relevant validation checks to ensure the issue is resolved and no new errors were introduced.
      7.  **Loop:** Repeat the cycle: Validate → Identify Issues → Plan Fixes → Execute Fixes → Re-Validate, until no further issues remain or you've reached the best possible state using your tools and judgment. Any encountered issues that are genuinely ambiguous or require conceptual clarification (which should be rare) may be escalated to the user only if you cannot make a reasonable judgment. You do not need to ask the user before continuing the loop. Only stop or escalate if you hit an issue that cannot reasonably be resolved without domain expertise.

      If your understanding of the dataset changes significantly, use SetBasicMetadata to update the Title, Description, or structure_notes.

      **Key Areas for Validation and Refinement:**
      *   **Required Fields:** Ensure all REQUIRED DwC fields (based on the chosen core) are present and populated.
      *   **Data Types & Formats:**
          *   eventDate: Must be ISO 8601.
          *   decimalLatitude: Numeric, between -90 and 90.
          *   decimalLongitude: Numeric, between -180 and 180.
          *   individualCount: Whole positive integers.
          *   Other numeric fields are valid numbers.
      *   **Controlled Vocabularies:**
          *   basisOfRecord: Must be from the allowed list.
          *   occurrenceStatus: ('present', 'absent').
          *   Other DwC terms with controlled vocabularies (e.g., sex, lifeStage if used).
      *   **Data Integrity & Consistency:**
          *   occurrenceID (or taxonID in Checklist core) must be unique if it's a core table.
          *   If a MeasurementOrFact extension is used, its linking ID (e.g., occurrenceID) must correspond to valid IDs in the core table.
          *   Consistency in scientificName usage.
          *   Look for and attempt to clean common issues like leading/trailing whitespace, inconsistent capitalization (especially in scientificName or locality), or mixed data types within a column that should be uniform.
      *   **Common Structural Patterns:** Re-check for any structural issues missed earlier that become apparent after transformation (e.g., a column that should have been numeric but contains text that prevented conversion).

      Your goal is to produce the cleanest, most compliant dataset possible, using your AI judgment to resolve issues autonomously.

- model: api.task
  fields:
    name: Final Review & Publication
    text: |-
      This is the last step before attempting to publish the user's data to GBIF.

      **Your Responsibilities:**
      1.  **Summarize Work Done:** Based on the structural_notes, briefly explain to the user the key transformations and validations that have been performed on their data to get it to this stage. Highlight any major changes or assumptions made. 
      2.  **Present Final Metadata:** Show the user the current Title and Description for the dataset. Ask for their final confirmation or if they'd like any last-minute changes to these. Use SetBasicMetadata if changes are made.
      3.  **Request Publication Approval:** Clearly ask the user if they are ready to proceed with publishing their dataset to GBIF.org (Test instance). Emphasise that this is the testing version of GBIF, and their data is not visible on the main site. 
      4.  **Initiate Publication:** If the user approves:
          *   Call the PublishDwC tool. This tool will generate the Darwin Core Archive, upload it, and register the dataset with GBIF.
      5.  **Report Outcome:**
          *   If successful, present the GBIF dataset URL to the user.
          *   If PublishDwC returns an error, report this error clearly to the user.

      Ensure the user understands this is the final step before their data (potentially) becomes publicly accessible via GBIF's test system.
